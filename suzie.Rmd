---
title: "Comparison of comparisons"
author: "Suzie Brown"
date: "22 November 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(BradleyTerry2)
library(StatRank)
library(DescTools)
```

## Define functions
```{r}
# to simulate ratings-type data
make.ratings <- function(n.alts, n.raters, strengths=NULL, sd.strengths=1, sd.noise=0.1) {
  if (!is.null(strengths) && length(strengths)!=n.alts){stop("length of strengths vector is not equal to number of alternatives")}
  if (is.null(strengths)){strengths <- rnorm(n.alts, 0, sd.strengths)}
  ratings <- replicate(n.raters, strengths + rnorm(n.alts, 0, sd.noise))
  list(ratings=ratings, strengths=strengths)
}

# to convert ratings to rankings
ratings2ranking <- function(ratings) {
  invrank <- apply(ratings, 2, order, decreasing=TRUE)
  rank <- apply(invrank, 2, order)
  rank
}

# to convert rankings to pairwise summary
ranking2pairs <- function(ranking) {
  n.alts <- nrow(ranking)
  w <- matrix(NA, nrow=n.alts, ncol=n.alts)
  for (i in 1:n.alts) {
    for (j in 1:n.alts) {
      w[i,j] <- sum(ranking[i,]<ranking[j,])
    }
  }
  w
}

# to normalise the strength vectors
normalise <- function(x, k=2) {
  xcentre <- x - mean(x)
  xcentre/sum(xcentre^k)
}
```

# Preliminary experiments

## Simulate some data and convert to each data type
```{r}
n.alts <- 20
foo <- make.ratings(n.alts=n.alts, n.raters=10, strengths=1:n.alts, sd.noise=5)
strengths <- foo$strengths
true.ranks <- order(order(strengths, decreasing=TRUE))

ratings <- foo$ratings
ranks <- ratings2ranking(ratings)
pairs <- ranking2pairs(ranks)
rownames(pairs) <- 1:nrow(pairs)
colnames(pairs) <- 1:nrow(pairs)
```

## Fit each model to the corresponding data type

### Ratings-type data (linear model)
```{r}
s.hat.ratings <- rowMeans(ratings) # estimated strengths (MLE)
r.hat.ratings <- order(order(s.hat.ratings, decreasing=TRUE))# estimated rankings
```

### Rankings-type data (Plackett-Luce model)
```{r}
rev.ranks <- apply(ranks,2,order, decreasing=TRUE)
PLfit <- Estimation.PL.MLE(t(rev.ranks))
s.hat.ranks <- log(PLfit$Mean) # we need to log the output from PL
r.hat.ranks <- order(order(PLfit$Mean, decreasing=TRUE))
```

### Pair comparison-type data (Bradley-Terry model)
```{r}
pairs.long <- countsToBinomial(pairs) # change contingency table to tall data format
BTfit <- BTm(outcome=as.matrix(pairs.long[,3:4]), player1=pairs.long[,1], player2=pairs.long[,2])
s.hat.pairs <- c(0,BTfit$coefficients)
r.hat.pairs <- order(order(c(1,as.vector(BTfit$coefficients)), decreasing=T))
```

## Compare the (normalised) estimated strengths
Some information has been lost in converting data types, so it is not possible to recover the absolute strengths. We can recover relative strengths. To compare the output of each method we will centre and scale the strength vectors.
```{r}
S.true <- scale(strengths)
S.rating <- scale(s.hat.ratings)
S.pair <- scale(s.hat.pairs)
S.rank <- scale(s.hat.ranks)
```
Now make a plot.
```{r}
plot(S.true, type='l', lwd=2, col="grey", ylim=range(S.true)+0.5*c(-1,1), ylab="Strength", main="Estimated strength of each alternative")
points(S.rating, type='l', lwd=2, col=2)
points(S.rank, type='l', lwd=2, col=4)
points(S.pair, type='l', lwd=2, col="purple")
legend("topleft", c("true", "ratings: LM", "ranks: PL", "pairs: BT"), lwd=2, col=c('grey',2,4,'purple'), lty=1)
```

## Performance Metrics
Kendall's tau distance is the number of discordant pairs between two vectors; we will use this as a distance measure (metric?) for rankings.
```{r}
rankdist.pairs <- ConDisPairs(table(true.ranks, r.hat.pairs))$D
rankdist.ranks <- ConDisPairs(table(true.ranks, r.hat.ranks))$D
rankdist.ratings <- ConDisPairs(table(true.ranks, r.hat.ratings))$D

MSE.pairs <- mean((strengths-s.hat.pairs)^2)
MSE.ranks <- mean((strengths-s.hat.ranks)^2)
MSE.ratings <- mean((strengths-s.hat.ratings)^2)

bias.pairs <- mean(strengths-s.hat.pairs)
bias.ranks <- mean(strengths-s.hat.ranks)
bias.ratings <- mean(strengths-s.hat.ratings)

perftable <- data.frame(rankdist=c(rankdist.ratings,rankdist.ranks, rankdist.pairs), strengthMSE=c(MSE.ratings,MSE.ranks,MSE.pairs), strengthbias=c(bias.ratings,bias.ranks, bias.pairs))
rownames(perftable) <- c("Ratings: LM", "Ranks: PL", "Pairs: BT")
perftable
```

# Further Experiments

## Varying number of alternatives and raters, and amount of noise
```{r}
par(mfrow=c(3,3), mar=rep(1,4))
for (n.alts in c(5,20,100)){ 
for (n.raters in c(5,20,100)) {
  #make data
  foo <- make.ratings(n.alts=n.alts, n.raters=n.raters, strengths=1:n.alts, sd.noise=5)
  strengths <- foo$strengths
  true.ranks <- order(order(strengths, decreasing=TRUE))
  ratings <- foo$ratings
  ranks <- ratings2ranking(ratings)
  pairs <- ranking2pairs(ranks)
  rownames(pairs) <- 1:nrow(pairs)
  colnames(pairs) <- 1:nrow(pairs)
  
  # linear model (ratings-data)
  s.hat.ratings <- rowMeans(ratings) # estimated strengths (MLE)
  r.hat.ratings <- order(order(s.hat.ratings, decreasing=TRUE))# estimated rankings
  
  #PL model (ranks-data)
  rev.ranks <- apply(ranks,2,order, decreasing=TRUE)
  PLfit <- Estimation.PL.MLE(t(rev.ranks))
  s.hat.ranks <- log(PLfit$Mean) # we need to log the output from PL
  r.hat.ranks <- order(order(PLfit$Mean, decreasing=TRUE))
  
  # BT model (pairs-data)
  pairs.long <- countsToBinomial(pairs) # change contingency table to tall data format
  BTfit <- BTm(outcome=as.matrix(pairs.long[,3:4]), player1=pairs.long[,1], player2=pairs.long[,2])
  s.hat.pairs <- c(0,BTfit$coefficients)
  r.hat.pairs <- order(order(c(1,as.vector(BTfit$coefficients)), decreasing=T))
  
  #normalise strengths
  S.true <- scale(strengths)
  S.rating <- scale(s.hat.ratings)
  S.pair <- scale(s.hat.pairs)
  S.rank <- scale(s.hat.ranks)
  
  #make the plot
  # plot(S.true, type='l', lwd=2, col="grey", ylim=range(S.true)+0.5*c(-1,1), ylab="Strength", main=paste("n.alts=", n.alts, ", n.raters=", n.raters))
  # points(S.rating, type='l', lwd=1, col=2)
  # points(S.rank, type='l', lwd=1, col=4)
  # points(S.pair, type='l', lwd=1, col="purple")
  # legend("topleft", c("true", "LM", "PL", "BT"), lwd=1, col=c('grey',2,4,'purple'), lty=1, cex=0.5)
  
  #print the summary table
  rankdist.pairs <- ConDisPairs(table(true.ranks, r.hat.pairs))$D
  rankdist.ranks <- ConDisPairs(table(true.ranks, r.hat.ranks))$D
  rankdist.ratings <- ConDisPairs(table(true.ranks, r.hat.ratings))$D
  MSE.pairs <- mean((S.true-S.pair)^2)
  MSE.ranks <- mean((S.true-S.rank)^2)
  MSE.ratings <- mean((S.true-S.rating)^2)
  # bias.pairs <- mean(S.true-S.pair)
  # bias.ranks <- mean(S.true-S.rank)
  # bias.ratings <- mean(S.true-S.rating)
  perftable <- data.frame(rankdist=c(rankdist.ratings,rankdist.ranks, rankdist.pairs), strengthMSE=c(MSE.ratings,MSE.ranks,MSE.pairs))
  rownames(perftable) <- c("Ratings: LM", "Ranks: PL", "Pairs: BT")
  print(perftable)
}}
```

## Now a more interesting distribution of strengths
Strengths will be clustered so that some are eaiser to tell apart than others.
We will vary the amount of noise.
```{r}
pdf("file.pdf",width=12,height=10) 
par(mfrow=c(2,2), mar=rep(2,4))
n.alts <- 20
n.raters <- 10
strengths <- runif(n.alts) #set true strength parameters

#initialise variables
count <- 0
rankdist.ratings<-numeric(3)
rankdist.ranks<- numeric(3)
rankdist.pairs<-numeric(3)
MSE.ratings<-numeric(3)
MSE.ranks<-numeric(3)
MSE.pairs<-numeric(3)
bias.ratings<-numeric(3)
bias.ranks<-numeric(3)
bias.pairs<-numeric(3)

for (sd.noise in c(0.01, 0.05, 0.1, 0.5)) {
  count <- count + 1
  
  #make data
  foo <- make.ratings(n.alts=n.alts, n.raters=n.raters, strengths=strengths, sd.noise=sd.noise)
  strengths <- foo$strengths
  true.ranks <- order(order(strengths, decreasing=TRUE))
  ratings <- foo$ratings
  ranks <- ratings2ranking(ratings)
  pairs <- ranking2pairs(ranks)
  rownames(pairs) <- 1:nrow(pairs)
  colnames(pairs) <- 1:nrow(pairs)
  
  # linear model (ratings-data)
  s.hat.ratings <- rowMeans(ratings) # estimated strengths (MLE)
  r.hat.ratings <- order(order(s.hat.ratings, decreasing=TRUE))# estimated rankings
  
  #PL model (ranks-data)
  rev.ranks <- apply(ranks,2,order, decreasing=TRUE)
  PLfit <- Estimation.PL.MLE(t(rev.ranks))
  s.hat.ranks <- log(PLfit$Mean) # we need to log the output from PL
  r.hat.ranks <- order(order(PLfit$Mean, decreasing=TRUE))
  
  # BT model (pairs-data)
  pairs.long <- countsToBinomial(pairs) # change contingency table to tall data format
  BTfit <- BTm(outcome=as.matrix(pairs.long[,3:4]), player1=pairs.long[,1], player2=pairs.long[,2])
  s.hat.pairs <- c(0,BTfit$coefficients)
  r.hat.pairs <- order(order(c(1,as.vector(BTfit$coefficients)), decreasing=T))
  
  #normalise strengths & put in order to make plots more interpretable
  S.true <- scale(strengths)[order(strengths)]
  S.rating <- scale(s.hat.ratings)[order(strengths)]
  S.pair <- scale(s.hat.pairs)[order(strengths)]
  S.rank <- scale(s.hat.ranks)[order(strengths)]
  
  #make the plot
  plot(S.true, type='l', lwd=4, col="grey", ylim=range(S.true)+0.5*c(-1,1), ylab="Strength", main=paste("n.alts=", n.alts, ", n.raters=", n.raters, ", sd.noise=", sd.noise))
  points(S.rating, type='l', lwd=1, col=2)
  points(S.rank, type='l', lwd=1, col=4)
  points(S.pair, type='l', lwd=1, col="purple")
  legend("topleft", c("true", "LM", "PL", "BT"), lwd=c(4,1,1,1), col=c('grey',2,4,'purple'), lty=1, cex=1)
  
  # calculate performance metrics
  rankdist.pairs[count] <- ConDisPairs(table(true.ranks, r.hat.pairs))$D
  rankdist.ranks[count] <- ConDisPairs(table(true.ranks, r.hat.ranks))$D
  rankdist.ratings[count] <- ConDisPairs(table(true.ranks, r.hat.ratings))$D
  MSE.pairs[count] <- mean((S.true-S.pair)^2)
  MSE.ranks[count] <- mean((S.true-S.rank)^2)
  MSE.ratings[count] <- mean((S.true-S.rating)^2)
  # bias.pairs[count] <- mean(S.true-S.pair) #bias is not useful, always tiny due to normalising
  # bias.ranks[count] <- mean(S.true-S.rank)
  # bias.ratings[count] <- mean(S.true-S.rating)
}
dev.off()
```
And print the performance statistics from each run.
```{r}
for (count in 1:4) {
  perftable <- data.frame(Kendall=c(rankdist.ratings[count],rankdist.ranks[count], rankdist.pairs[count]), MSE=c(MSE.ratings[count],MSE.ranks[count],MSE.pairs[count]))
  rownames(perftable) <- c("Ratings: LM", "Ranks: PL", "Pairs: BT")
  print(perftable)
}
```

# What about ties?
We'll assume that the strengths are still continuous, so the ratings data won't contain any ties.
But we'll add a mechanism for ties to arise in the rankings and pair-comparisons. In particular, if two ratings are within $\delta$ of each other, they will be tied in the corresponding ranking. The parameter $\delta$ sets the "prevalence" of ties.

## Redefine functions so they can produce ties
```{r}
ratings2ranking <- function(ratings, allow.ties=FALSE, delta=NULL) {
  invrank <- apply(ratings, 2, order, decreasing=TRUE)
  rank <- apply(invrank, 2, order)
  if (allow.ties) {
    if (!is.numeric(delta)) {stop("please provide threshold delta for tied ranks")}
    rank <- apply(ratings, 2, rank.tie, delta=delta)
  }
  rank
}

rank.tie <- function(ratings, delta) {
  rank <- order(order(ratings, decreasing=TRUE))
  rank.tie <- rank
  for (k in 1:(length(ratings)-1)){
    which.tied <- which(rank>k & abs(ratings-ratings[rank==k])<delta)
    if (length(which.tied)==0) {next} # no ties with this one
    rank.tie[which.tied] <- rank.tie[rank==k]
  }
  rank.tie
}

ratings2pairs <- function(rating, allow.ties=FALSE, delta=NULL) {
  n.alts <- nrow(rating)
  w <- matrix(NA, nrow=n.alts, ncol=n.alts)
  for (i in 1:n.alts) {
    for (j in 1:n.alts) {
      if (allow.ties){
        if (!is.numeric(delta)) {stop("please provide threshold delta for tied comparisons")}
        w[i,j] <- sum(rating[i,] > rating[j,] + delta)
      }
      else{
        w[i,j] <- sum(rating[i,]>rating[j,])
      }
    }
  }
  w
}
```

```{r}
pdf("file.pdf",width=12,height=10) 
par(mfrow=c(2,2), mar=rep(2,4))
n.alts <- 20
n.raters <- 10
sd.noise <- 0.1
strengths <- runif(n.alts) #set true strength parameters

#initialise variables
count <- 0
rankdist.ratings<-numeric(3)
rankdist.ranks<- numeric(3)
rankdist.pairs<-numeric(3)
MSE.ratings<-numeric(3)
MSE.ranks<-numeric(3)
MSE.pairs<-numeric(3)
pties.ratings<-numeric(3)
pties.ranks<-numeric(3)
pties.pairs<-numeric(3)

for (delta in c(0.005, 0.01, 0.05, 0.1)) {
  count <- count + 1
  
  #make data
  foo <- make.ratings(n.alts=n.alts, n.raters=n.raters, strengths=strengths, sd.noise=sd.noise)
  strengths <- foo$strengths
  true.ranks <- order(order(strengths, decreasing=TRUE))
  ratings <- foo$ratings
  ranks <- ratings2ranking(ratings, allow.ties=TRUE, delta=delta)
  pairs <- ratings2pairs(ratings, allow.ties=TRUE, delta=delta)
  rownames(pairs) <- 1:nrow(pairs)
  colnames(pairs) <- 1:nrow(pairs)
  
  # linear model (ratings-data)
  s.hat.ratings <- rowMeans(ratings) # estimated strengths (MLE)
  r.hat.ratings <- order(order(s.hat.ratings, decreasing=TRUE))# estimated rankings
  
  #PL model (ranks-data) should work still with ties
  rev.ranks <- apply(ranks,2,order, decreasing=TRUE)
  PLfit <- Estimation.PL.MLE(t(rev.ranks))
  s.hat.ranks <- log(PLfit$Mean) # we need to log the output from PL
  r.hat.ranks <- order(order(PLfit$Mean, decreasing=TRUE))
  
  #BT model (pairs-data) doesn't work with ties
  pairs.long <- countsToBinomial(pairs) # change contingency table to tall data format
  BTfit <- BTm(outcome=as.matrix(pairs.long[,3:4]), player1=pairs.long[,1], player2=pairs.long[,2])
  s.hat.pairs <- c(0,BTfit$coefficients)
  r.hat.pairs <- order(order(c(1,as.vector(BTfit$coefficients)), decreasing=T))
  
  #normalise strengths & put in order to make plots more interpretable
  S.true <- scale(strengths)[order(strengths)]
  S.rating <- scale(s.hat.ratings)[order(strengths)]
  S.pair <- scale(s.hat.pairs)[order(strengths)]
  S.rank <- scale(s.hat.ranks)[order(strengths)]
  
  #make the plot
  plot(S.true, type='l', lwd=4, col="grey", ylim=range(S.true)+0.5*c(-1,1), ylab="Strength", main=paste("n.alts=", n.alts, ", n.raters=", n.raters, ", delta=", delta))
  points(S.rating, type='l', lwd=1, col=2)
  points(S.rank, type='l', lwd=1, col=4)
  points(S.pair, type='l', lwd=1, col="purple")
  legend("topleft", c("true", "LM", "PL", "BT"), lwd=c(4,1,1,1), col=c('grey',2,4,'purple'), lty=1, cex=1)
  
  # calculate performance metrics
  rankdist.pairs[count] <- ConDisPairs(table(true.ranks, r.hat.pairs))$D
  rankdist.ranks[count] <- ConDisPairs(table(true.ranks, r.hat.ranks))$D
  rankdist.ratings[count] <- ConDisPairs(table(true.ranks, r.hat.ratings))$D
  MSE.pairs[count] <- mean((S.true-S.pair)^2)
  MSE.ranks[count] <- mean((S.true-S.rank)^2)
  MSE.ratings[count] <- mean((S.true-S.rating)^2)
  pties.ratings[count] <- 0
  pties.ranks.fun <- function(ranks){sum(duplicated(ranks) | duplicated(ranks, fromLast=TRUE))}
  pties.ranks[count] <- sum(apply(ranks, 2, pties.ranks.fun))/length(ranks)
  pairs.noties <- ratings2pairs(ratings)
  pties.pairs[count] <- sum(pairs.noties - pairs)/sum(pairs.noties)
}
dev.off()
```

And again print the performance statistics from each run.
```{r}
for (count in 1:4) {
  perftable <- data.frame(Kendall=c(rankdist.ratings[count],rankdist.ranks[count], rankdist.pairs[count]), MSE=c(MSE.ratings[count],MSE.ranks[count],MSE.pairs[count]), ties=c(pties.ratings[count], pties.ranks[count], pties.pairs[count]))
  rownames(perftable) <- c("Ratings: LM", "Ranks: PL", "Pairs: BT")
  print(perftable)
}
```
